<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MIT6.824 - Lecture - II</title>
    <link href="/2021/03/04/MIT6-824-Lecture-II/"/>
    <url>/2021/03/04/MIT6-824-Lecture-II/</url>
    
    <content type="html"><![CDATA[<h2 id="Lecture-06-Raft1"><a href="#Lecture-06-Raft1" class="headerlink" title="Lecture 06 Raft1"></a>Lecture 06 Raft1</h2><h3 id="脑裂（Split-Brain）"><a href="#脑裂（Split-Brain）" class="headerlink" title="脑裂（Split Brain）"></a>脑裂（Split Brain）</h3><ul><li>MapReduce复制了计算，但是复制这个动作，或者说整个MapReduce被一个单主节点控制。</li><li>GFS以主备（primary-backup）的方式复制数据。它会实际的复制文件内容。但是它也依赖一个单主节点，来确定每一份数据的主拷贝的位置。</li><li>VMware FT，它在一个Primary虚机和一个Backup虚机之间复制计算相关的指令。但是，当其中一个虚机出现故障时，为了能够正确的恢复。需要一个Test-and-Set服务来确认，Primary虚机和Backup虚机只有一个能接管计算任务。</li></ul><p>以上三个都是多副本系统，但都只由一个单节点Master决定哪个副本是Primary；使用一个单节点决定Primary的好处：无法自己去做否决，需要用test-and-set服务去决定，但是会遇上单点故障；使用单点是为了避免脑裂</p><p>多副本的Test-and-set为什么很难避免脑裂<br>在VWmare FT场景下，假设有两个T&amp;S服务器（S1、S2）以及两个客户端（C1、C2），当S1无法和C2通信、S2无法和C1通信，这时服务器会认为这两个服务器宕机了，于是只给能通信的客户端设为Primary，这时两个客户端都成为了Primary而无需与另一方协商。<br>抉择：要么等待两个服务器能够正常通信，但这样等于没有容错性；要么不等待全部服务器，但会导致脑裂</p><p>早期的解决方法：</p><ul><li>构建一个不出现故障的网络（电脑内连接了CPU和内存的线路就是不可能出现故障的网络）</li><li>人工解决问题（如果只有一个服务器响应，则提醒维护人员检查服务器是否关机了，还是网络问题）</li></ul><h3 id="过半票决（Majority-Vote）"><a href="#过半票决（Majority-Vote）" class="headerlink" title="过半票决（Majority Vote）"></a>过半票决（Majority Vote）</h3><p>当网络出现故障，将网络分割成两半，网络的两边独自运行，且不能访问对方，这通常被称为<strong>网络分区</strong>。</p><p>服务器总数为奇数：如果系统有 2 * F + 1 个服务器，那么系统最多可以接受F个服务器出现故障，仍然可以正常工作。</p><p>过半票决的一半指的是全部服务器数量的一半，而不是当前开机的服务器的数量的一半</p><p>系统总是需要过半的服务器才能完成任何操作，任何操作需要过半的服务器进行批准，例如Raft Leader的选举，新Leader竞选成功必定有过半的服务器批准，而过半的服务器必定有一些（至少一个）服务器拥有旧Leader的任期号或提交的操作</p><h3 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h3><p>Raft以库的形式存在于服务。一个基于Raft的多副本服务的每个副本由应用程序代码和Raft库构成。应用程序代码接受RPC或其他客户端请求，不同节点的Raft库相互协作，维护多副本之间的状态同步。</p><p>对于Lab3来说，Raft节点上层应用是KV数据库，Raft层帮助应用程序将其状态（KV TABLE）拷贝到其他副本中，KV数据库通过调用Raft库函数，来传递自己的状态和Raft反馈的信息。Raft本身也会保存状态，最重要的就是Raft记录操作的日志。</p><h5 id="工作流程："><a href="#工作流程：" class="headerlink" title="工作流程："></a>工作流程：</h5><ul><li>客户端发送请求至Raft集群中的Leader节点（如Put、Get）</li><li>Leader节点的应用程序将操作发送到它的Raft层，让Raft把操作提交到多副本的日志中，并在完成时通知它</li><li>Raft节点之间相互交互，直到过半的节点将这个新操作加入到它们的日志中</li><li>当Raft的Leader节点知道过半的节点已完成日志更新，则会告知应用程序已经提交副本，可以开始它的操作，KV数据库开始它的操作</li><li>当请求被提交到客户端后，Leader告诉其它节点他们的响应已经被提交</li></ul><h5 id="Raft日志"><a href="#Raft日志" class="headerlink" title="Raft日志"></a>Raft日志</h5><p>Raft Log的作用：</p><ul><li>Log是Leader用来对操作排序的一种手段。对于复制状态机，所有副本不仅要执行相同的操作，还要以相同的顺序执行这些操作</li><li>对于Raft的Follower来说，Log是用来存放临时操作的地方。当Follower收到来自leader的新的commit号后才执行操作</li><li>Leader需要在它的Log中记录操作，因为这些操作可能需要重传给Follower（网络问题或者宕机）</li><li>持久化存储，帮助重启的服务器恢复状态。你可能的确需要一个故障了的服务器在修复后，能重新加入到Raft集群</li></ul><h5 id="应用层接口"><a href="#应用层接口" class="headerlink" title="应用层接口"></a>应用层接口</h5><p>应用层和Raft层之间的接口<br>两个接口：</p><ul><li>应用层转发客户端请求的接口（key-value层说：我接到了这个请求，请把它存在Log中，并在committed之后告诉我。）</li><li>Raft Leader完成提交后通知应用层的接口（你刚刚在Start函数中传给我的请求已经commit了）（请求和请求在Log的位置）</li></ul><h4 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h4><p>Paxos系统没有Leader：通过一组服务器来共同认可Log的顺序，进而构成一致性系统</p><p>Raft有Leader的原因：若服务器不出现故障，Leader的存在使得系统更加高效（无Leader系统需要先进行一轮信息交互确定一个临时Leader）</p><p>使用任期号（term number）来区分不同的Leader</p><p>Leader的创建：每个Raft节点有一个选举定时器，如果在定时器结束之前没有收到Leader的消息，则认为Leader已经下线并开始选举；当定时器时间耗尽，会开始一轮选举，当前服务器会增加任期号，并发起请求投票RPC，发给所有的Raft节点。（若有过半的服务器故障了，或者网络分区内的服务器不过半，则选不出Leader）<br>当服务器收到过半服务器认可投票后，该服务器直接知道自己成了Leader，但其他服务器需要收到新Leader发送的特定任期号的AppendEntries后才会知道选举成功。AppendEntries会重启服务器的选举定时器</p><p>选举失败的情况</p><ul><li><p>过半的服务器故障或者网络分区内没有过半的服务器</p></li><li><p>所有的服务器选举定时器同时耗尽时间，同时发起选举，分割了选票（解决：随即选举定时器的超时时间，选举定时器的超时时间需要至少大于Leader的心跳间隔）</p><p>选举定时器超时时间的设定</p></li><li><p>选举定时器的超时时间需要至少大于Leader的心跳间隔</p></li><li><p>选举定时器的超时时间越长，系统集群瘫痪的恢复时间也就越长</p></li><li><p>不同节点的选举定时器的超时时间差至少需要大于发送一条RPC所需要的往返（Round-Trip）时间。使得第一个开始选举的节点能够完成选举。</p></li></ul><h2 id="Lecture-07-Raft2"><a href="#Lecture-07-Raft2" class="headerlink" title="Lecture 07 Raft2"></a>Lecture 07 Raft2</h2>]]></content>
    
    
    
    <tags>
      
      <tag>分布式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lettcode小结</title>
    <link href="/2021/03/02/Lettcode/"/>
    <url>/2021/03/02/Lettcode/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
    <tags>
      
      <tag>Lettcode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT6.824 - Lecture - I</title>
    <link href="/2021/02/28/MIT6-824-Lecture-I/"/>
    <url>/2021/02/28/MIT6-824-Lecture-I/</url>
    
    <content type="html"><![CDATA[<h2 id="Lecture-01-引言"><a href="#Lecture-01-引言" class="headerlink" title="Lecture 01 - 引言"></a>Lecture 01 - 引言</h2><h3 id="分布式系统的目的："><a href="#分布式系统的目的：" class="headerlink" title="分布式系统的目的："></a>分布式系统的目的：</h3><ul><li>更高的计算性能</li><li>容错</li><li>地理分布</li><li>安全（代码分布式运行）？</li></ul><h3 id="分布式系统挑战："><a href="#分布式系统挑战：" class="headerlink" title="分布式系统挑战："></a>分布式系统挑战：</h3><ul><li>并发带来的复杂交互和时间依赖(同步、异步)问题</li><li>部分故障</li><li>性能难以计算（达到1000台计算机的计算性能需要多少计算机？）</li></ul><h3 id="构建分布式系统工具："><a href="#构建分布式系统工具：" class="headerlink" title="构建分布式系统工具："></a>构建分布式系统工具：</h3><ul><li>RPC远程过程调用</li><li>Thread线程</li><li>并发控制</li></ul><h3 id="分布式系统特性"><a href="#分布式系统特性" class="headerlink" title="分布式系统特性"></a>分布式系统特性</h3><ul><li>Scalability可扩展性（分布式系统如何扩展，如web服务器、数据库）</li><li>Availability可用性（系统容错、在特定故障范围内，系统仍能够提供服务）</li><li>Recoverability可恢复性（非易失存储如硬盘、系统副本（问题在于系统副本会与原系统偏离同步，需通过管理多副本来容错））</li><li>Consistency一致性（故障导致的非一致性，所以有弱一致性（不保证数据最新）、强一致性通信成本过高，弱一致性提高性能）</li></ul><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce的思想是，应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。</p><p>Job。整个MapReduce计算称为Job。<br>Task。每一次MapReduce调用称为Task。<br>对于一个完整的MapReduce Job，它由一些Map Task和一些Reduce Task组成</p><p>单词计数器例子：<br>首先将数据分为多个数据块，对于每个数据块执行Map操作，统计每个数据块各个单词的数量；执行完之后进行Reduce操作，不同Reduce程序收集不同的单词的数量，从而得到所有单词的数量</p><p><font color=#CF0000 >MapReduce框架有两个步骤（MapReduce 框架其实包含5 个步骤：Map、Sort、Combine、Shuffle 以及Reduce。这5 个步骤中最重要的就是Map 和Reduce。这也是和Spark 最相关的两步</font></p><p><font color=#CF0000 >Map 和 Reduce 中间夹杂着一步数据移动，也就是 shuffle，这步操作会涉及数量巨大的网络传输（network I/O），需要耗费大量的时间。由于 MapReduce 的框架限制，一个 MapReduce 任务只能包含一次 Map 和一次 Reduce，计算完成之后，MapReduce 会将运算结果写回到磁盘中（更准确地说是分布式存储系统）供下次计算使用。如果所做的运算涉及大量循环，比如估计模型参数的梯度下降或随机梯度下降算法就需要多次循环使用训练数据，那么整个计算过程会不断重复地往磁盘里读写中间结果。这样的读写数据会引起大量的网络传输以及磁盘读写，极其耗时，而且它们都是没什么实际价值的废操作。因为上一次循环的结果会立马被下一次使用，完全没必要将其写入磁盘。<br></font></p><p><font color=#CF0000 >mapReduce适合对实时性要求不高的计算（如对处理速度不敏感的离线批处理，计算时中间结果溢写到磁盘），io消耗大。spark是基于内存的计算框架，计算速度是很快的</font></p><font color=#0000FF >spark 的优点：<ul><li>虽然它的计算模式也属于map - Reduce，但又不局限于map - Reduce。</li><li>提供内存计算，中间结果直接放到内存中，带来了更高的迭代运算</li><li>spark基于DAG（有向无环图）的任务调度机制优于mapreduce</font></li></ul><h2 id="Lecture-03-GFS-谷歌文件系统"><a href="#Lecture-03-GFS-谷歌文件系统" class="headerlink" title="Lecture 03 - GFS 谷歌文件系统"></a>Lecture 03 - GFS 谷歌文件系统</h2><h3 id="分布式存储系统难点"><a href="#分布式存储系统难点" class="headerlink" title="分布式存储系统难点"></a>分布式存储系统难点</h3><ul><li>为了性能而将数据分片（分块存储于多台服务器，并行读取）</li><li>分片，故障是常态，需要一个自动容错系统</li><li>为了容错，对数据进行备份，而数据备份常常会出现不一致的问题</li><li>为了一致性需要服务器频繁的交互，导致低性能</li></ul><h3 id="GFS设计目标"><a href="#GFS设计目标" class="headerlink" title="GFS设计目标"></a>GFS设计目标</h3><ul><li>大型快速的文件系统</li><li>全局通用的存储系统</li><li>数据分割存储（可多台服务器读取一个文件、可存储比单个磁盘还要大的文件）</li><li>能够自我修复</li><li>其它特征：GFS只在一个数据中心运行、内部系统（只面向谷歌员工）、大型的顺序文件读写定制（GFS不支持随机访问？）</li></ul><h3 id="GFS-Master"><a href="#GFS-Master" class="headerlink" title="GFS Master"></a>GFS Master</h3><ul><li>主从模式，一个Master节点（保存了文件名和存储位置的对应关系）工作，大量Chunk服务器。Master用来管理文件和Chunk的信息，Chunk用于存储实际数据。</li><li>存储的数据，文件名 =&gt; Chunk ID（数据块ID）=&gt;Chunk数据（每个chunk存储在哪些服务器、Chunk版本号、<font color=#9F0000 >哪个Chunk服务器持有主Chunk(所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一)、主Chunk的租约过期时间(主Chunk只能在特定的租约时间内担任主Chunk)</font>）</li><li>Master故障，以上存在内存的数据都丢失，因此Master会将数据以log形式存储在磁盘<br>（其中文件名=&gt;Chunk ID需要存储，Chunk存在哪些服务器可不用存（Master重启后可轮询）、主Chunk相关的也可以不用存在磁盘，版本号是否要存取决于GFS的工作方式）</li><li><font color=#9F0000 >在磁盘中维护log而不是数据库的原因是，数据库本质上来说是某种B树（b-tree）或者hash table，相比之下，追加log会非常的高效，因为你可以将最近的多个log记录一次性的写入磁盘。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入。所以使用Log可以使得磁盘写入更快一些。</font><br>关于B树在存储中的作用和优点：<a href="https://blog.csdn.net/xrymibz/article/details/72466630">https://blog.csdn.net/xrymibz/article/details/72466630</a><br>二叉树，平衡二叉树，红黑树，b树，b+树，b*树的缺点与优点以及使用场景：<a href="https://blog.csdn.net/ff_simon/article/details/101055134">https://blog.csdn.net/ff_simon/article/details/101055134</a></li></ul><h3 id="GFS-读文件"><a href="#GFS-读文件" class="headerlink" title="GFS 读文件"></a>GFS 读文件</h3><ul><li>第一步是客户端（或者应用程序）将文件名和偏移量（特定偏移位置和特定长度的数据）发送给Master。</li><li>第二步，Master节点将Chunk Handle（也就是ID，记为H）和服务器列表发送给客户端。</li><li>第三步，客户端可以从服务器列表中选择一个chunk服务器读取数据，选择网络上最近的服务器，并将读请求发送给此服务器。</li><li>第四步，客户端与选出的Chunk服务器通信，将Chunk Handle和偏移量发送给那个Chunk服务器。Chunk服务器根据文件名找出对应的Chunk文件，根据偏移量读出对应的数据段，返回给客户端。（如果数据跨越边界，GFS的库会将请求拆分成多个，向Master多次请求，最终可能向多个Chunk服务器请求数据）</li></ul><h3 id="GFS-写文件"><a href="#GFS-写文件" class="headerlink" title="GFS 写文件"></a>GFS 写文件</h3><ul><li>客户端希望将buffer中数据追加到某个文件，请求Master返回该文件最后一个Chunk的位置（多个客户端写文件，其中每个客户端都无法得知文件有多长，它们不知道其他客户端写了多少，因此无法通过偏移量或者某个Chunk追加数据）</li><li>写文件需要通过Chunk的Primary主副本来写入，所以需要考虑主副本是否存在</li><li>若主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器（版本号和Master中记录的一致，不能简单地将所有Chunk服务器中最新的版本号作为Master版本号，因为存有最新版本号的Chunk服务器可能出现故障，所以最新版本号必须在Master以持久化形式存储）</li><li>Master找出所有最新副本Chunk服务器，将其中一个作为Primary，其他作为Secondary，并增加版本号，并将Primary、Secondary和版本号信息告知这些存有最新副本的服务器，这些服务器会保存这些信息以在故障后仍能报告Master版本信息</li><li>通过Primary接受客户端的写请求，并将写请求应用在其他副本Chunk服务器中，管理Chunk的版本号使得Master可以将更新Chunk的能力代管给Primary服务器。Master故障后仍能在同样的服务器上进行更新。</li><li>Master会给Primary一个定时的租约，确保不会出现两个Primary（脑裂Split Brain）</li><li>任何一个Secondary更新失败，Primary都会报告失败，客户端会重新发起追加的过程，但是已更新的副本不会回滚。</li></ul><h3 id="将GFS升级为一致性系统"><a href="#将GFS升级为一致性系统" class="headerlink" title="将GFS升级为一致性系统"></a>将GFS升级为一致性系统</h3><ul><li>探测重复的能力，防止重复的数据被写入</li><li>如果Primary要求Secondary执行一个操作，Secondary必须完成，否则被移除</li><li>Primary要求Secondary追加数据时，直到Primary确信所有Secondary完成追加之前，Secondary不能将数据暴漏给读请求（写请求两阶段提交，第一阶段Primary向Secondary发送请求，并等待所有Secondary回复，第二阶段Secondary全部回复完成，Primary告知Client完成了写操作）</li><li>旧的Primary在未完成第二阶段写操作时崩溃了，新Primary需要显式的与Secondary进行同步以确保操作历史的结尾是相同的。</li><li><font color=#9F0000 >Secondary之间可能会有差异，或者客户端从Master节点获取的是稍微过时的Secondary。系统要么需要将所有的读请求都发送给Primary，因为只有Primary知道哪些操作实际发生了，要么对于Secondary需要一个租约系统，就像Primary一样，这样就知道Secondary在哪些时间可以合法的响应客户端。</font></li></ul><h3 id="GFS只有一个Master节点的缺点"><a href="#GFS只有一个Master节点的缺点" class="headerlink" title="GFS只有一个Master节点的缺点"></a>GFS只有一个Master节点的缺点</h3><ul><li>Master为每个文件和每个Chunk维护表单，随着数据量增大，单个Master节点的内存空间成为瓶颈</li><li>单个Master节点难以处理成千上万个客户端的请求</li><li>应用程序难以处理GFS奇怪的语义（允许多个副本数据存在不一致）</li><li>GFS的Master节点出现故障后，需要人工切换，时间较长</li></ul><h2 id="Lecture-04-VMware-FT-容错"><a href="#Lecture-04-VMware-FT-容错" class="headerlink" title="Lecture 04 - VMware FT 容错"></a>Lecture 04 - VMware FT 容错</h2><p>FailStop:出现错误则停止运行，而非计算出错误结果<br>复制能处理的故障：网络中断、服务器停机<br>不能处理的故障：软件的BUG和硬件设计的缺陷<br>如果复制有关联则没有帮助：如服务器在同一地点遭遇了断电、过热、自然灾害等<br>复制的成本是否值得</p><h3 id="状态转移和复制状态机"><a href="#状态转移和复制状态机" class="headerlink" title="状态转移和复制状态机"></a>状态转移和复制状态机</h3><ul><li>状态转移：将Primary内存的内容复制到BackUp</li><li>复制状态机：将来自客户端的操作或者其他外部事件，从Primary传输到Backup。复制状态机操作量更小，但更复杂</li><li>Primary和Backup出现状态不一致后，Primary授权给Chunk1服务器租约后宕机，Backup接手后会另外指定一个租约服务器，出现两个Primary Chunk服务器，脑裂</li><li>Vmware FT论文中只讨论了单核情况下的复制状态机，面对多核和并行计算，状态转移更加健壮。</li><li><font color=#9F0000 >vWmare复制了所有的内存和寄存器，而GFS只复制被应用程序抽象后的Chunk和Chunk ID。VWmare效率低但是通用性强，从机器级别开始复制，可以在该机器上运行任何软件（？？？） </font></li></ul><h3 id="VWmare-FT-工作原理"><a href="#VWmare-FT-工作原理" class="headerlink" title="VWmare FT 工作原理"></a>VWmare FT 工作原理</h3><ul><li>两个虚拟机Primary Backup互为副本，Primary到Backup之间同步的数据流的通道称之为Log Channel，从Primary发往Backup的事件被称为Log Channel上的Log Event/Entry</li></ul><p>工作流程：</p><ul><li>客户端向Primary发送请求（网络数据包）</li><li>网络数据包产生一个中断（？），中断送到了VMM，VMM识别这是一个发送给多副本服务的一个输入</li><li>VMM做两件事 1）在虚拟机的Guest操作系统中，模拟数据包到达的中断，将数据发送给应用程序的Primary副本 2）将这个数据包拷贝一份发送给Backup所在的VMM</li><li>Backup所在的VMM也会模拟数据包到达的中断，把数据发送给应用程序的Backup副本</li><li>虚拟机内的服务回复客户端的请求，产生回复报文，通过虚拟机虚拟的虚拟网卡发出，VMM将这个报文回复给客户端；Backup也会做上述操作，但是到达它所在的VMM时，报文不会发送回去，所以只有Primary虚拟机产生了回复报文给客户端</li><li>Primary产生故障后，Backup因为一段时间后未收到Primary发送给他的Log Event，于是Backup上线，告知客户端将请求发往Backup（新的Primary）；Backup产生故障，则Primary抛弃它，停止向它发送事件，变成了一个单点服务</li><li></li></ul><h3 id="非确定性事件（Non-Deterministic-Events）"><a href="#非确定性事件（Non-Deterministic-Events）" class="headerlink" title="非确定性事件（Non-Deterministic Events）"></a>非确定性事件（Non-Deterministic Events）</h3><p>计算机中每一个指令并不都是由计算机内存的内容而确定，指令在Primary和Backup运行结果可能不同，即非确定性事件</p><p>非确定性事件分类</p><ul><li>客户端输入：对于Primary和Backup，客户端输入最好要在相同的时间，相同的位置触发，否则执行过程就是不一样的，进而会导致它们的状态产生偏差。（数据包的内容、数据包到达与中断的时间）</li><li>有些指令在不同计算机上结果不同：随机数生成器、获取当前时间、获取计算机唯一ID</li><li>多CPU的并发，指令在不同的CPU上会交织运行，产生的指令顺序不可预测（VWmare未讨论）</li></ul><p>处理方法：Primary将网络数据包和指令序号发送给Backup，Backup虚拟机的VMM在对应指令序号的位置模拟一个网卡中断发送给Backup虚机；怪异指令也同样用某种方法让Backup虚机得到的结果和Primary一致</p><h3 id="输出控制"><a href="#输出控制" class="headerlink" title="输出控制"></a>输出控制</h3><ul><li>VMware FT系统的输出：对于客户端请求的响应</li><li>只有Primary虚机才会真正的将回复送出，而Backup虚机只是将回复简单的丢弃掉。</li><li>控制输出（Output Rule）：直到Backup虚机确认收到了相应的Log条目，Primary虚机才能将回复客户端。（应付情况：自增后Primary回复了客户端，但Backup还未自增，Primary就崩溃，在下一次自增时，客户端得到同样的数据）</li><li>这种同步等待限制了复制系统的性能</li></ul><h3 id="重复输出"><a href="#重复输出" class="headerlink" title="重复输出"></a>重复输出</h3><p>使用TCP解决了重复输出的问题（<font color=#9F0000 >当Backup接管服务时，因为它的状态与Primary相同，所以它知道TCP连接的状态和TCP传输的序列号。当Backup生成回复报文时，这个报文的TCP序列号与之前Primary生成报文的TCP序列号是一样的，这样客户端的TCP栈会发现这是一个重复的报文，它会在TCP层面丢弃这个重复的报文，用户层的软件永远也看不到这里的重复。） </font></p><h3 id="Test-and-Set服务"><a href="#Test-and-Set服务" class="headerlink" title="Test-and-Set服务"></a>Test-and-Set服务</h3><p>Primary和Backup之间网络通信出现了故障，试图上线接管对方的服务<br>解决：向一个外部的第三方权威机构求证，来决定Primary还是Backup允许上线。这里的第三方就是Test-and-Set服务。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/02/28/hello-world/"/>
    <url>/2021/02/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
